{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e667a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from collections import Counter\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import statistics as st\n",
    "from tcn import TCN\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95add4bb",
   "metadata": {},
   "source": [
    "Data preparation - Binning dwell times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data using pandas\n",
    "data = pd.read_csv(\"browsing_train.csv\") \n",
    "# View first rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dwell times for every row\n",
    "data['dwell_times'] = abs(data['server_timestamp_epoch_ms'].diff(-1))\n",
    "# View first rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614306e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate amount of zero millisecond dwell times in the total dataset\n",
    "zeros = data['dwell_times'].value_counts()[0.0]\n",
    "rows = data.shape[0]\n",
    "percentageofzeros = round(((zeros/rows)*100),2)\n",
    "\n",
    "print(\"Number of zero dwell times: {}, number of dwell times: {}, percentage of zero milliseconds dwell times: {}\".format(zeros,rows,percentageofzeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this dataset into csv file\n",
    "df = pd.DataFrame(data) \n",
    "df.to_csv('browsing_train_dwelltimes.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ddbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sessions = []\n",
    "current_session_id = None\n",
    "current_session = []\n",
    "\n",
    "# Read dataset\n",
    "with open(\"browsing_train_dwelltimes.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for idx, row in enumerate(reader):\n",
    "            \n",
    "        # Row will contain: session_id_hash, product_action, product_sku_hash\n",
    "        _session_id_hash = row['session_id_hash']\n",
    "        # When a new session begins, store the old one and start again\n",
    "        if current_session_id and current_session and _session_id_hash != current_session_id:\n",
    "            user_sessions.append(current_session)\n",
    "            # Resets session\n",
    "            current_session = []\n",
    "        # We extract events from session\n",
    "        if row['product_action'] == '' and row['event_type'] ==  'pageview':\n",
    "            current_session.append('view')\n",
    "\n",
    "        elif row['product_action'] != '':\n",
    "            current_session.append(row['product_action'])\n",
    "        \n",
    "        # Append dwell times between actions\n",
    "        if row['dwell_times'] == '':\n",
    "            current_session.append(\"nan\")\n",
    "        else:\n",
    "            current_session.append(float(row['dwell_times']))\n",
    "            \n",
    "        # Update the current session id\n",
    "        current_session_id = _session_id_hash\n",
    "        \n",
    "\n",
    "# Print how many sessions we have\n",
    "print(\"# total sessions: {}\".format(len(user_sessions)))\n",
    "# Print first session\n",
    "print(\"First session is: {}\".format(user_sessions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76489b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the last value of a user session is of float value, delete this value\n",
    "# As this value is the time between sessions\n",
    "for sublist in user_sessions:\n",
    "    if type(sublist[-1]) == float or (sublist[-1]) == \"nan\":\n",
    "        del(sublist[-1])\n",
    "    else:\n",
    "        continue\n",
    "assert not any( x[-1] == float and x[-1] == \"nan\" for x in user_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first session again\n",
    "print(user_sessions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe21e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The calculated time between sessions are very large\n",
    "# Before we continue, we need to figure out the largest dwell time that is actually\n",
    "# inter-click\n",
    "floatvalues = []\n",
    "for session in user_sessions:\n",
    "    for item in session:\n",
    "        if type(item) == float:\n",
    "            floatvalues.append(item)\n",
    "        else:\n",
    "            continue\n",
    "largestdwelltime = max(floatvalues)\n",
    "print(\"Largest inter-click dwell time:\", largestdwelltime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c47633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import original data again\n",
    "data = pd.read_csv(\"browsing_train.csv\") \n",
    "# Calculate dwell times for every row\n",
    "data['dwell_times'] = abs(data['server_timestamp_epoch_ms'].diff(-1))\n",
    "# We change the time between sessions to the largest inter-click dwell time \n",
    "# This is necessary for the binning of dwell times later \n",
    "data.loc[data['dwell_times'] > largestdwelltime, 'dwell_times'] = largestdwelltime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19309f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data['dwell_times'].max() == largestdwelltime:\n",
    "    print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin dwell times into quartiles\n",
    "data[\"dwell_bins\"] = pd.qcut(\n",
    "    x=data[\"dwell_times\"],\n",
    "    q = 5,\n",
    "    labels=[\"Very low\", \"Low\", \"Medium\", \"High\"],\n",
    "    duplicates = \"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7145a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b59b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print edge values of bins\n",
    "print(data['dwell_bins'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print labels of bins\n",
    "print(data['dwell_bins'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda23d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this dataset into csv file again\n",
    "df = pd.DataFrame(data) \n",
    "df.to_csv('browsing_train_dwelltimes.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd398e0",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae00fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sessions = []\n",
    "current_session_id = None\n",
    "current_session = []\n",
    "\n",
    "with open(\"browsing_train_dwelltimes.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for idx, row in enumerate(reader):\n",
    "            \n",
    "        # Row will contain: session_id_hash, product_action, product_sku_hash\n",
    "        _session_id_hash = row['session_id_hash']\n",
    "        # When a new session begins, store the old one and start again\n",
    "        if current_session_id and current_session and _session_id_hash != current_session_id:\n",
    "            user_sessions.append(current_session)\n",
    "            # Resets session\n",
    "            current_session = []\n",
    "        # We extract events from session\n",
    "        if row['product_action'] == '' and row['event_type'] ==  'pageview':\n",
    "            current_session.append('view')\n",
    "\n",
    "        elif row['product_action'] != '':\n",
    "            current_session.append(row['product_action'])\n",
    "        \n",
    "        # Append binned dwell times between actions\n",
    "        if row['dwell_bins'] == '':\n",
    "            current_session.append(\"nan\")\n",
    "        else:\n",
    "            current_session.append(row['dwell_bins'])\n",
    "            \n",
    "        # update the current session id\n",
    "        current_session_id = _session_id_hash\n",
    "        \n",
    "\n",
    "# Print how many sessions we have\n",
    "print(\"# total sessions: {}\".format(len(user_sessions)))\n",
    "# Print first session\n",
    "print(\"First session is: {}\".format(user_sessions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the last value of a user session is a binned dwell time, delete this value\n",
    "# This is the time between sessions\n",
    "labels=[\"Very low\", \"Low\", \"Medium\", \"High\", \"Very high\"]\n",
    "for sublist in user_sessions:\n",
    "    if (sublist[-1]) in labels or (sublist[-1]) == \"nan\":\n",
    "        del(sublist[-1])\n",
    "    else:\n",
    "        continue\n",
    "assert not any( x[-1] in labels and x[-1] == \"nan\" for x in user_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert events to numbers and add start and stop token\n",
    "def session_indexed(s):\n",
    "    \"\"\"\n",
    "    Converts a session (of actions) to indices and adds start/end tokens\n",
    "    :param s: list of actions in a session (i.e 'add','detail', etc)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    action_to_idx = {'start': 0, 'end': 1, 'add': 2, 'remove': 3, 'detail': 4, 'view': 5,\n",
    "                    'Very low': 6, 'Low': 7, 'Medium': 8, 'High': 9}\n",
    "    return [action_to_idx['start']] + [action_to_idx[e] for e in s] + [action_to_idx['end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_sessions = []\n",
    "abandon_sessions = []\n",
    "browse_sessions = []\n",
    "for s in user_sessions:\n",
    "    # If add and purchase event in sessions and purchase event appears after add event...\n",
    "    if 'purchase' in s and 'add' in s and s.index('purchase') > s.index('add'):\n",
    "        p_session = s\n",
    "        # Remove purchase event and dwell time before purchase event\n",
    "        p_session = (p_session[:p_session.index(\"purchase\")][:-1])\n",
    "            \n",
    "        # Remove clickstreams shorter than 9 or longer than 309 clicks and dwell times\n",
    "        if len(p_session) < 9 or len(p_session) > 309:\n",
    "            continue\n",
    "        else:\n",
    "            # Append to list\n",
    "            purchase_sessions.append(p_session)\n",
    "        # Assert not any purchase event left in clickstream    \n",
    "        assert not any( e == 'purchase' for e in p_session)\n",
    "\n",
    "    # If add event and no purchase event in session...    \n",
    "    elif 'add' in s and not 'purchase' in s:\n",
    "        if len(s) < 9 or len(s) > 309:\n",
    "            continue\n",
    "        else:\n",
    "            abandon_sessions.append(s)\n",
    "            \n",
    "    # If no purchase event in session...        \n",
    "    elif 'purchase' not in s:\n",
    "        if len(s) < 9 or len(s) > 309:\n",
    "            continue\n",
    "        else:    \n",
    "            browse_sessions.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start stop token, convert to numbers\n",
    "purchase_sessions = [session_indexed(s) for s in purchase_sessions]\n",
    "abandon_sessions = [session_indexed(s) for s in abandon_sessions]\n",
    "browse_sessions = [session_indexed(s) for s in browse_sessions]\n",
    "\n",
    "# Combine sessions into final dataset\n",
    "x = purchase_sessions + abandon_sessions + browse_sessions\n",
    "\n",
    "# give label=1 for purchase, label=0 for abandon, label=2 for browse\n",
    "y = [1]*len(purchase_sessions) +[0]*len(abandon_sessions) + [2]*len(browse_sessions)\n",
    "assert len(x) == len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f77092",
   "metadata": {},
   "source": [
    "Split data into train, val and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# First, split the data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(x,y, train_size=0.7, stratify = y, random_state = 3340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd53dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, split the remaining data into a validation and test set\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5, stratify = y_rem, random_state = 3340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98083c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training data: {}, validation data: {}, test data: {}\".format(len(X_train), len(X_valid), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c73f60",
   "metadata": {},
   "source": [
    "Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8175028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training set to seperate sets per class\n",
    "def converttosessions(X_train, y_train):\n",
    "    tupletrainitems = [tuple(x) for x in X_train]\n",
    "    tuplex = tuple(tupletrainitems)\n",
    "    tupley = tuple(y_train)\n",
    "    newdic = zip(tupley,tuplex)\n",
    "    \n",
    "    abandon_sessions = []\n",
    "    purchase_sessions = []\n",
    "    browsing_sessions = []\n",
    "    \n",
    "    for x in list(newdic):\n",
    "        if x[0] == 0:\n",
    "            a = list(x[1])\n",
    "            b = a[1:-1]\n",
    "            abandon_sessions.append(b)\n",
    "        elif x[0] == 1:\n",
    "            a = list(x[1])\n",
    "            b = a[1:-1]\n",
    "            purchase_sessions.append(b)\n",
    "        elif x[0] == 2:\n",
    "            a = list(x[1])\n",
    "            b = a[1:-1]\n",
    "            browsing_sessions.append(b)\n",
    "    \n",
    "    return abandon_sessions, purchase_sessions, browsing_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a845da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abandon_sessions, train_purchase_sessions, train_browsing_sessions = converttosessions(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72211cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count dwell times per class\n",
    "def dwelltimesperclass(abandon_sessions, purchase_sessions, browsing_sessions):\n",
    "    abandoncounter = Counter(itertools.chain(*abandon_sessions))\n",
    "    abandoncounter = dict(sorted(abandoncounter.items(), key=lambda item: item[0]))\n",
    "    purchasecounter = Counter(itertools.chain(*purchase_sessions))\n",
    "    purchasecounter = dict(sorted(purchasecounter.items(), key=lambda item: item[0]))\n",
    "    browsingcounter = Counter(itertools.chain(*browsing_sessions))\n",
    "    browsingcounter = dict(sorted(browsingcounter.items(), key=lambda item: item[0]))\n",
    "    \n",
    "    abandon_list = list(abandoncounter.values())\n",
    "    abandon_list = abandon_list[4:]\n",
    "    purchase_list = list(purchasecounter.values())\n",
    "    purchase_list = purchase_list[4:]\n",
    "    browsing_list = list(browsingcounter.values())\n",
    "    browsing_list = browsing_list[3:]\n",
    "    \n",
    "\n",
    "    print(\"Total number of events and dwell times per class:\")\n",
    "    print(\"Abandon: {}, purchase: {}, browsing: {}\".format(abandoncounter, purchasecounter, browsingcounter))\n",
    "    print(\"\\n\")\n",
    "    print(abandon_list, purchase_list, browsing_list)\n",
    "    total_dwell_abandon = sum(abandon_list)\n",
    "    total_dwell_purchase = sum(purchase_list)\n",
    "    total_dwell_browsing = sum(browsing_list)\n",
    "    \n",
    "    perc_verylowabandon = round(((abandon_list[0]/total_dwell_abandon) * 100),2)\n",
    "    perc_verylowpurchase = round(((purchase_list[0]/total_dwell_purchase) * 100),2)\n",
    "    perc_verylowbrowsing = round(((browsing_list[0]/total_dwell_browsing) * 100),2)    \n",
    "    print(\"Percentage of very low dwell times in abandon: {}, purchase: {}, browsing: {}\".format(perc_verylowabandon, perc_verylowpurchase, perc_verylowbrowsing))\n",
    "    \n",
    "    perc_lowabandon = round(((abandon_list[1]/total_dwell_abandon) * 100),2)\n",
    "    perc_lowpurchase = round(((purchase_list[1]/total_dwell_purchase) * 100),2)\n",
    "    perc_lowbrowsing = round(((browsing_list[1]/total_dwell_browsing) * 100),2)\n",
    "    print(\"Percentage of low dell times in abandon: {}, purchase: {}, browsing: {}\".format(perc_lowabandon, perc_lowpurchase, perc_lowbrowsing))\n",
    "    \n",
    "    perc_mediumabandon = round(((abandon_list[2]/total_dwell_abandon) * 100),2)\n",
    "    perc_mediumpurchase = round(((purchase_list[2]/total_dwell_purchase) * 100),2)\n",
    "    perc_mediumbrowsing = round(((browsing_list[2]/total_dwell_browsing) * 100),2)\n",
    "    print(\"Percentage of medium dwell times in abandon: {}, purchase: {}, browsing: {}\".format(perc_mediumabandon, perc_mediumpurchase, perc_mediumbrowsing))\n",
    "    \n",
    "    perc_highabandon = round(((abandon_list[3]/total_dwell_abandon) * 100),2)\n",
    "    perc_highpurchase = round(((purchase_list[3]/total_dwell_purchase) * 100),2)\n",
    "    perc_highbrowsing= round(((browsing_list[3]/total_dwell_browsing) * 100),2)\n",
    "    print(\"Percentage of high dwell times in abandon: {}, purchase: {}, browsing: {}\".format(perc_highabandon, perc_highpurchase, perc_highbrowsing))\n",
    "    \n",
    "    verylow_percentages = [perc_verylowabandon, perc_verylowpurchase, perc_verylowbrowsing]\n",
    "    low_percentages = [perc_lowabandon, perc_lowpurchase, perc_lowbrowsing]\n",
    "    medium_percentages = [perc_mediumabandon, perc_mediumpurchase, perc_mediumbrowsing]\n",
    "    high_percentages = [perc_highabandon, perc_highpurchase, perc_highbrowsing]\n",
    "    \n",
    "    df = pd.DataFrame({'Class': [\"Abandon\", \"Purchase\", \"Browsing-Only\"], 'Very Low': verylow_percentages, 'Low': low_percentages,\n",
    "                      'Medium': medium_percentages, 'High': high_percentages})\n",
    "    print(df)\n",
    "    df.plot(x='Class', kind='bar', stacked=True)\n",
    "    plt.legend(bbox_to_anchor=(1.0, 1.0))\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dwelltimesperclass(train_abandon_sessions, train_purchase_sessions, train_browsing_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise clickstream\n",
    "def visualizeclickstream(abandon_sessions, purchase_sessions, browsing_sessions):\n",
    "    abandon_clickstream = 0\n",
    "    purchase_clickstream = 0\n",
    "    browsing_clickstream = 0\n",
    "    length = list(range(1,39+1))\n",
    "    \n",
    "    for x in abandon_sessions:\n",
    "        if len(x) == length[-1] and 3 in x:\n",
    "            abandon_clickstream = x\n",
    "    for x in purchase_sessions:\n",
    "        if len(x) == length[-1] and 3 in x:\n",
    "            purchase_clickstream = x\n",
    "    for x in browsing_sessions:\n",
    "        if len(x) == length[-1] and 3 in x:\n",
    "            browsing_clickstream = x\n",
    "\n",
    "    # Plot a simple line chart\n",
    "    plt.figure(figsize=(8,4))\n",
    "\n",
    "    # Plot another line on the same chart/graph\n",
    "    plt.plot(length, purchase_clickstream, marker='p', color = 'tab:orange', linestyle='--')\n",
    "    plt.axhline(y = 5.5, linestyle = '-', color = 'black')\n",
    "\n",
    "    \n",
    "    #{add': 2, 'remove': 3, 'detail': 4, 'view': 5, 'Very low': 6, 'Low': 7, 'Medium': 8, 'High': 9}\n",
    "    y = [2,3,4,5,6,7,8,9]\n",
    "    yticks = ['Add', 'Remove', 'Detail', 'View', \"Very low\", 'Low', 'Medium', 'High']\n",
    "    xticks = list(range(0,41, 5))\n",
    "    plt.yticks(y, yticks)\n",
    "    plt.xticks(xticks)\n",
    "    \n",
    "    #Invert y-axis for legible plot\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend(['Purchase'])\n",
    "    plt.xlabel(\"Number of Clicks and Inter-click Dwell Times\")\n",
    "    plt.ylabel(\"Dwell Time                 Event Type\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d901b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeclickstream(train_abandon_sessions, train_purchase_sessions, train_browsing_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fff1a1",
   "metadata": {},
   "source": [
    "Padding & one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_len = 311\n",
    "X_train = pad_sequences(X_train, padding=\"post\",value=10, maxlen=max_len)\n",
    "X_valid = pad_sequences(X_valid, padding=\"post\", value=10, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, padding=\"post\", value=10, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to one-hot\n",
    "X_train = tf.one_hot(X_train, depth=11)\n",
    "X_valid = tf.one_hot(X_valid, depth=11)\n",
    "X_test = tf.one_hot(X_test, depth=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to arrays\n",
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "#One-hot encode labels\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_valid = to_categorical(y_valid, 3)\n",
    "\n",
    "# Save for metric calculations\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ec819",
   "metadata": {},
   "source": [
    "Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e6411",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accf6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnngridsearch(filter_size, neurons, kernel_size):\n",
    "    batch = 32\n",
    "    epochs = 20\n",
    "    patience = 5\n",
    "    l = 0.001\n",
    "    #Hyperparameters\n",
    "    opt = keras.optimizers.Adam(l)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # Convolutional layer\n",
    "    model.add(Conv1D(filter_size, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size = 2))    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffefbc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to be tested\n",
    "neurons = [32, 64, 128]\n",
    "kernel_num = [3, 5, 7]\n",
    "filter_num = [32, 64, 128]\n",
    "f1_scores = dict()\n",
    "\n",
    "for n in neurons:\n",
    "    print(\"Testing neurons:\", n)\n",
    "    for k in kernel_num:\n",
    "        print(\"Testing kernel size:\", k)\n",
    "        for f in filter_num:\n",
    "            print(\"Testing filters\", f)\n",
    "            print(\"Fitting model\")\n",
    "            cnn = cnngridsearch(f, n, k)\n",
    "            \n",
    "            #Calculating y_pred\n",
    "            y_pred_validate = cnn.predict(X_valid)\n",
    "            rounded = np.argmax(np.round(y_pred_validate),axis=1)\n",
    "            rounded = list(rounded)\n",
    "        \n",
    "            #Evaluating the model\n",
    "            f1score = f1_score(valid_labels, rounded, average = \"macro\")\n",
    "            print(\"Macro-averaged F1 score:\", f1score)\n",
    "        \n",
    "            #Appending evaluations to dictionaries\n",
    "            f1_scores[f1score] = (f, n, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee40328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort F1 scores from highest to lowest\n",
    "sortscores = {key: val for key, val in sorted(f1_scores.items(), key = lambda ele: ele[0])}\n",
    "print(\"Result dictionary sorted by F1 score : \" + str(sortscores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8bdd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model using only one stack\n",
    "def tcngridsearch(filter_size, kernel_size, dilations):\n",
    "    epochs=20\n",
    "    patience=5\n",
    "    batch=32\n",
    "    l=0.001\n",
    "    \n",
    "    #Hyperparameters\n",
    "    opt = keras.optimizers.Adam(l)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "\n",
    "    # Define Model\n",
    "    model = keras.Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # TCN layer\n",
    "    model.add(TCN(\n",
    "        nb_filters= filter_size,\n",
    "        kernel_size=kernel_size,\n",
    "        dilations=dilations\n",
    "        ))\n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26220070",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_num = [3, 5, 7]\n",
    "filter_num = [32, 64, 128]\n",
    "dilations = [[1, 2, 4, 8, 16, 32, 64, 128], [1, 2, 4, 8, 16, 32, 64]]\n",
    "f1_scores = dict()\n",
    "\n",
    "# Calculate receptive field\n",
    "print(\"Receptive fields\")\n",
    "print(\"For kernel size 3: {}\".format(3*1*128))\n",
    "print(\"For kernel size 5: {}\".format(5*1*64))\n",
    "print(\"For kernel size 7: {}\".format(7*1*64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in filter_num:\n",
    "    print(\"Testing filter size:\", f)\n",
    "    for k in kernel_num:\n",
    "        print(\"Testing kernel size:\", k)\n",
    "        if k == 3:\n",
    "            d = dilations[0]\n",
    "            print(\"Last dilation:\", d[-1])\n",
    "        if k == 5 or k == 7:\n",
    "            d = dilations[1]\n",
    "            print(\"Last dilation:\", d[-1])\n",
    "        \n",
    "        print(\"Fitting model\")\n",
    "        tcn = tcngridsearch(f, k, d)\n",
    "            \n",
    "        #Calculating y_pred\n",
    "        y_pred_validate = tcn.predict(X_valid)\n",
    "        rounded = np.argmax(np.round(y_pred_validate),axis=1)\n",
    "        rounded = list(rounded)\n",
    "        \n",
    "        #Evaluating the model\n",
    "        f1score = f1_score(valid_labels, rounded, average = \"macro\")\n",
    "        print(\"f1 score for this model:\", f1score)\n",
    "        \n",
    "        #Appending evaluations to dictionaries\n",
    "        f1_scores[f1score] = (f, k, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a73d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort F1 scores from highest to lowest\n",
    "sortscores = {key: val for key, val in sorted(f1_scores.items(), key = lambda ele: ele[0])}\n",
    "print(\"Result dictionary sorted by F1 score : \" + str(sortscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36730d4d",
   "metadata": {},
   "source": [
    "Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores and configure confusion matrices\n",
    "def metric_calculation(predictions):\n",
    "    rounded = np.argmax(np.round(predictions),axis=1)\n",
    "    rounded = list(rounded)\n",
    "    f1 = f1_score(test_labels, rounded, average='macro')\n",
    "    print (\"f1macro: {}\".format(round(f1, 3)))\n",
    "    print(metrics.classification_report(test_labels, rounded, digits=3))\n",
    "    \n",
    "    target_names = [\"Abandon\", \"Purchase\", \"Browsing-Only\"]\n",
    "    cm = metrics.confusion_matrix(test_labels, rounded)\n",
    "    cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    heatmap = sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=target_names, yticklabels=target_names, cmap=plt.cm.Blues)\n",
    "    plt.ylabel('Actual Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    \n",
    "    figure = heatmap.get_figure()    \n",
    "    figure.savefig('cm.png')\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcddf3d",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "def lstmmodel(X_train, y_train, X_val, y_val):\n",
    "    #Hyperparamaters\n",
    "    lr = 0.001\n",
    "    batch = 32\n",
    "    epochs = 50\n",
    "    patience = 10\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # LSTM layer\n",
    "    model.add(LSTM(64)) \n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmpredictions(X_test):\n",
    "    lstm = lstmmodel(X_train, y_train, X_valid, y_valid)\n",
    "    preds = lstm.predict(X_test,batch_size=32)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a7bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_preds = lstmpredictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_calculation(lstm_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed160833",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN baseline\n",
    "def cnnmodel(X_train, y_train, X_val, y_val):\n",
    "    lr = 0.001\n",
    "    batch = 32\n",
    "    epochs = 50\n",
    "    patience = 10\n",
    "    #Hyperparameters\n",
    "    opt = keras.optimizers.Adam(learning_rate = lr)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # Convolutional layer\n",
    "    model.add(Conv1D(filters = 64, kernel_size = 7, activation='relu'))\n",
    "    # Pooling layer\n",
    "    model.add(MaxPooling1D(pool_size = 2))    \n",
    "    # Flatten layer\n",
    "    model.add(Flatten())\n",
    "    # Fully connected layer\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60aa4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnpredictions(X_test):\n",
    "    cnn = cnnmodel(X_train, y_train, X_valid, y_valid)\n",
    "    preds = cnn.predict(X_test,batch_size=32)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60299926",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_preds = cnnpredictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_calculation(cnn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6b1e3",
   "metadata": {},
   "source": [
    "CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstmdwelltimes(X_train, y_train, X_val, y_val):\n",
    "    lr = 0.001\n",
    "    batch = 32\n",
    "    epochs = 50\n",
    "    patience = 10\n",
    "    #Hyperparameters\n",
    "    opt = keras.optimizers.Adam(learning_rate = lr)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # Convolutional layer\n",
    "    model.add(Conv1D(filters = 64, kernel_size = 7, activation='relu'))\n",
    "    # Pooling layer\n",
    "    model.add(MaxPooling1D(pool_size = 2))  \n",
    "    # LSTM layer\n",
    "    model.add(LSTM(64))\n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstmpredictions(X_test):\n",
    "    cnnlstm = cnnlstmmodel(X_train, y_train, X_valid, y_valid)\n",
    "    preds = cnnlstm.predict(X_test,batch_size=32)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlstm_preds = cnnlstmpredictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcfc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_calculation(cnnlstm_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2048b",
   "metadata": {},
   "source": [
    "TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcn_dwelltimes(X_train, y_train, X_val, y_val):\n",
    "    epochs=50\n",
    "    patience=10 \n",
    "    batch=32\n",
    "    lr=0.001\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate = lr)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "\n",
    "    # Define Model\n",
    "    model = keras.Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # TCN layer\n",
    "    model.add(TCN(\n",
    "        nb_filters=32,\n",
    "        kernel_size=3,\n",
    "        dilations=[1, 2, 4, 8, 16, 32, 64, 128]\n",
    "        ))\n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcnpredictions(X_test):\n",
    "    tcn = tcnmodel(X_train, y_train, X_valid, y_valid)\n",
    "    preds = tcn.predict(X_test,batch_size=32)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5033e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_preds = tcnpredictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_calculation(tcn_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
