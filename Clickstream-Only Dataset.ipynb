{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from collections import Counter\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import statistics as st\n",
    "from tcn import TCN\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7df6a5",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d24bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "user_sessions = []\n",
    "current_session_id = None\n",
    "current_session = []\n",
    "# Read dataset\n",
    "with open(\"browsing_train.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for idx, row in enumerate(reader):\n",
    "            \n",
    "        # Row will contain: session_id_hash, product_action, product_sku_hash\n",
    "         _session_id_hash = row['session_id_hash']\n",
    "         # When a new session begins, store the old one and start again\n",
    "        if current_session_id and current_session and _session_id_hash != current_session_id:\n",
    "            user_sessions.append(current_session)\n",
    "            # Resets session\n",
    "            current_session = []\n",
    "        # We extract events from session\n",
    "        if row['product_action'] == '' and row['event_type'] ==  'pageview':\n",
    "            current_session.append('view')\n",
    "\n",
    "        elif row['product_action'] != '':\n",
    "            current_session.append(row['product_action'])\n",
    "        # Update the current session id\n",
    "        current_session_id = _session_id_hash\n",
    "\n",
    "# Print how many sessions we have\n",
    "print(\"# total sessions: {}\".format(len(user_sessions)))\n",
    "# Print first session\n",
    "print(\"First session is: {}\".format(user_sessions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert events to numbers and add start and stop token\n",
    "def session_indexed(s):\n",
    "    \"\"\"\n",
    "    Converts a session (of actions) to indices and adds start/end tokens\n",
    "    :param s: list of actions in a session (i.e 'add','detail', etc)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    action_to_idx = {'start': 0, 'end': 1, 'add': 2, 'remove': 3, 'detail': 4, 'view': 5}\n",
    "    return [action_to_idx['start']] + [action_to_idx[e] for e in s] + [action_to_idx['end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fcbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_sessions = []\n",
    "abandon_sessions = []\n",
    "browse_sessions = []\n",
    "for s in user_sessions:\n",
    "    # If add and purchase event in sessions and purchase event appears after add event...\n",
    "    if 'purchase' in s and 'add' in s and s.index('purchase') > s.index('add'):\n",
    "        p_session = s\n",
    "        # Remove purchase event from session\n",
    "        p_session = (p_session[:p_session.index(\"purchase\")])\n",
    "            \n",
    "        # Remove clickstreams shorter than 5 or longer than 155 clicks\n",
    "        if len(p_session) < 5 or len(p_session) > 155:\n",
    "            continue\n",
    "        else:\n",
    "            # Append to list\n",
    "            purchase_sessions.append(p_session)\n",
    "        # Assert not any purchase event left in clickstream    \n",
    "        assert not any( e == 'purchase' for e in p_session)\n",
    "\n",
    "    # If add event and no purchase event in session...\n",
    "    elif 'add' in s and not 'purchase' in s:\n",
    "        if len(s) < 5 or len(s) > 155:\n",
    "            continue\n",
    "        else:\n",
    "            abandon_sessions.append(s)\n",
    "    # If no purchase event in session...    \n",
    "    elif 'purchase' not in s:\n",
    "        if len(s) < 5 or len(s) > 155:\n",
    "            continue\n",
    "        else:    \n",
    "            browse_sessions.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb9652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start stop token, convert to numbers\n",
    "purchase_sessions = [session_indexed(s) for s in purchase_sessions]\n",
    "abandon_sessions = [session_indexed(s) for s in abandon_sessions]\n",
    "browse_sessions = [session_indexed(s) for s in browse_sessions]\n",
    "\n",
    "# Combine sessions into final dataset\n",
    "x = purchase_sessions + abandon_sessions + browse_sessions\n",
    "\n",
    "# give label=1 for purchase, label=0 for abandon, label=2 for browse\n",
    "y = [1]*len(purchase_sessions) +[0]*len(abandon_sessions) + [2]*len(browse_sessions)\n",
    "assert len(x) == len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# total sessions after data prep: {}\".format(len(x)))\n",
    "percentage_left = 100 - (len(x)/len(user_sessions) *100)\n",
    "print(\"% total sessions left after data prep: {}\".format(round(len(x)/len(user_sessions)*100),2))\n",
    "print(\"% drop: {}\".format(round(percentage_left),2))\n",
    "print(\"\\n\")\n",
    "print(\"Remaining dataset:\")\n",
    "print(\"%abandon sessions: {}\".format(round(((len(abandon_sessions)/len(x))*100),2)))\n",
    "print(\"%purchase sessions: {}\".format(round(((len(purchase_sessions)/len(x))*100),2)))\n",
    "print(\"%browse sessions: {}\".format(round(((len(browse_sessions)/len(x))*100),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746f7cd",
   "metadata": {},
   "source": [
    "Split data into train, val and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1986ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# First, split the data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(x,y, train_size=0.7, stratify = y, random_state = 3340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, split the remaining data into a validation and test set\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5, stratify = y_rem, random_state = 3340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training data: {}, validation data: {}, test data: {}\".format(len(X_train), len(X_valid), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76fd94",
   "metadata": {},
   "source": [
    "Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training set to seperate sets per class\n",
    "def converttosessions(X_train, y_train):\n",
    "    tupletrainitems = [tuple(x) for x in X_train]\n",
    "    tuplex = tuple(tupletrainitems)\n",
    "    tupley = tuple(y_train)\n",
    "    newdic = zip(tupley,tuplex)\n",
    "    \n",
    "    abandon_sessions = []\n",
    "    purchase_sessions = []\n",
    "    browsing_sessions = []\n",
    "    \n",
    "    for x in list(newdic):\n",
    "        if x[0] == 0:\n",
    "            a = list(x[1])\n",
    "            b = a[1:-1]\n",
    "            abandon_sessions.append(b)\n",
    "        elif x[0] == 1:\n",
    "            a = list(x[1])\n",
    "            b = a[1:-1]\n",
    "            purchase_sessions.append(b)\n",
    "        elif x[0] == 2:\n",
    "            a = list(x[1])\n",
    "            b = a[1:-1]\n",
    "            browsing_sessions.append(b)\n",
    "    \n",
    "    return abandon_sessions, purchase_sessions, browsing_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bfcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abandon_sessions, train_purchase_sessions, train_browsing_sessions = converttosessions(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0577832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some statistics on the training dataset\n",
    "def statistics(X_train, abandon_sessions, purchase_sessions, browsing_sessions):\n",
    "    length_abandon = len(abandon_sessions)\n",
    "    length_purchase = len(purchase_sessions)\n",
    "    length_browsing = len(browsing_sessions)\n",
    "    print(\"Number of clickstreams per category, abandon: {}, purchase: {}, browsing: {}\".format(length_abandon, length_purchase, length_browsing))\n",
    "    \n",
    "    session_lengths = []\n",
    "    abandon_lengths = []\n",
    "    purchase_lengths = []\n",
    "    browsing_lengths = []\n",
    "    for x in X_train:\n",
    "        session_lengths.append(len(x)-2)\n",
    "    for x in abandon_sessions:\n",
    "        abandon_lengths.append(len(x))\n",
    "    for x in purchase_sessions:\n",
    "        purchase_lengths.append(len(x))\n",
    "    for x in browsing_sessions:\n",
    "        browsing_lengths.append(len(x))\n",
    "    \n",
    "    length_total = len(session_lengths)\n",
    "    print(\"Total number of clickstreams: {}\".format(length_total))\n",
    "    perc_abandon = round(((length_abandon/length_total) * 100),2)\n",
    "    perc_purchase = round(((length_purchase/length_total) * 100),2)\n",
    "    perc_browsing = round(((length_browsing/length_total) * 100),2)\n",
    "    print(\"Percentage of clickstreams per category, abandon: {}, purchase: {}, browsing: {}\".format(perc_abandon, perc_purchase, perc_browsing))\n",
    "    \n",
    "    \n",
    "    shortest_session = min(session_lengths)\n",
    "    longest_session = max(session_lengths)\n",
    "    average_session = round(st.mean(session_lengths))\n",
    "    median_session = int(st.median(session_lengths))\n",
    "    std_session = round(st.stdev(session_lengths))\n",
    "    \n",
    "    print(\"Shortest total session length: {}, longest total session length: {}\".format(shortest_session, longest_session))\n",
    "    print(\"Average total session length: {}, standard deviation: {}\".format(average_session, std_session))\n",
    "    print(\"Median total session length: {}\".format(median_session))\n",
    "    \n",
    "    shortest_abandon = min(abandon_lengths)\n",
    "    shortest_purchase = min(purchase_lengths)\n",
    "    shortest_browsing = min(browsing_lengths)\n",
    "    print(\"Shortest clickstream per category, abandon: {}, purchase: {}, browsing: {}\".format(shortest_abandon, shortest_purchase, shortest_browsing))\n",
    "          \n",
    "    longest_abandon = max(abandon_lengths)\n",
    "    longest_purchase = max(purchase_lengths)\n",
    "    longest_browsing = max(browsing_lengths)\n",
    "    print(\"Longest clickstream per category, abandon: {}, purchase: {}, browsing: {}\".format(longest_abandon, longest_purchase, longest_browsing))\n",
    "\n",
    "    average_total = round(st.mean(session_lengths), 2)\n",
    "    average_abandon = round(st.mean(abandon_lengths), 2)\n",
    "    average_purchase = round(st.mean(purchase_lengths), 2)\n",
    "    average_browsing = round(st.mean(browsing_lengths), 2)\n",
    "    print(\"Average clickstream length all sessions: {}\".format(average_total))\n",
    "    print(\"Average clickstream length per category, abandon: {}, purchase: {}, browsing: {}\".format(average_abandon, average_purchase, average_browsing))\n",
    "    \n",
    "    std_total = round(st.stdev(session_lengths), 2)\n",
    "    std_abandon = round(st.stdev(abandon_lengths), 2)\n",
    "    std_purchase = round(st.stdev(purchase_lengths), 2)\n",
    "    std_browsing = round(st.stdev(browsing_lengths), 2)\n",
    "    print(\"Standard deviation of mean total clickstreams: {}\".format(std_total))       \n",
    "    print(\"Standard deviation of mean clickstream per category, abandon: {}, purchase: {}, browsing: {}\".format(std_abandon, std_purchase, std_browsing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf56f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(X_train, train_abandon_sessions, train_purchase_sessions, train_browsing_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising three clickstreams\n",
    "def visualizeclickstream(abandon_sessions, purchase_sessions, browsing_sessions):\n",
    "    abandon_clickstream = 0\n",
    "    purchase_clickstream = 0\n",
    "    browsing_clickstream = 0\n",
    "    length = list(range(1,20+1))\n",
    "    \n",
    "    for x in abandon_sessions:\n",
    "        if len(x) == length[-1] and 3 in x:\n",
    "            abandon_clickstream = x\n",
    "    for x in purchase_sessions:\n",
    "        if len(x) == length[-1] and 3 in x:\n",
    "            purchase_clickstream = x\n",
    "    for x in browsing_sessions:\n",
    "        if len(x) == length[-1] and 3 in x:\n",
    "            browsing_clickstream = x\n",
    "\n",
    "    # Plot a simple line chart\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(length, abandon_clickstream, marker='s')\n",
    "\n",
    "    # Plot another line on the same chart/graph\n",
    "    plt.plot(length, purchase_clickstream, marker='p', linestyle='--')\n",
    "    \n",
    "    plt.plot(length, browsing_clickstream, marker='o', linestyle=':')\n",
    "\n",
    "    \n",
    "    #{add': 2, 'remove': 3, 'detail': 4, 'view': 5}\n",
    "    y = [2,3,4,5]\n",
    "    yticks = ['Add', 'Remove', 'Detail', 'View']\n",
    "    xticks = list(range(0,21, 2))\n",
    "    plt.yticks(y, yticks)\n",
    "    plt.xticks(xticks)\n",
    "    \n",
    "    #Invert y-axis for legible plot\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend(['Abandon', 'Purchase', 'Browsing-Only'])\n",
    "    plt.xlabel(\"Number of Clicks\")\n",
    "    plt.ylabel(\"Event Type\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeclickstream(train_abandon_sessions, train_purchase_sessions, train_browsing_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising events per class\n",
    "def eventsperclass(abandon_sessions, purchase_sessions, browsing_sessions):\n",
    "    abandoncounter = Counter(itertools.chain(*abandon_sessions))\n",
    "    abandoncounter = dict(sorted(abandoncounter.items(), key=lambda item: item[0]))\n",
    "    purchasecounter = Counter(itertools.chain(*purchase_sessions))\n",
    "    purchasecounter = dict(sorted(purchasecounter.items(), key=lambda item: item[0]))\n",
    "    browsingcounter = Counter(itertools.chain(*browsing_sessions))\n",
    "    browsingcounter = dict(sorted(browsingcounter.items(), key=lambda item: item[0]))\n",
    "    \n",
    "    abandon_list = list(abandoncounter.values())\n",
    "    purchase_list = list(purchasecounter.values())\n",
    "    browsing_list = list(browsingcounter.values())\n",
    "    \n",
    "    print(\"Total number of events per class:\")\n",
    "    print(\"Abandon: {}, purchase: {}, browsing: {}\".format(abandoncounter, purchasecounter, browsingcounter))\n",
    "    print(\"\\n\")\n",
    "    length_abandon = sum(abandon_list)\n",
    "    length_purchase = sum(purchase_list)\n",
    "    length_browsing = sum(browsing_list)\n",
    "    \n",
    "    #{add': 2, 'remove': 3, 'detail': 4, 'view': 5}\n",
    "    perc_addabandon = round(((abandon_list[0]/length_abandon) * 100),2)\n",
    "    perc_addpurchase = round(((purchase_list[0]/length_purchase) * 100),2)\n",
    "    print(\"Percentage of add events in abandon: {}, purchase: {}\".format(perc_addabandon, perc_addpurchase))\n",
    "    \n",
    "    perc_removeabandon = round(((abandon_list[1]/length_abandon) * 100),2)\n",
    "    perc_removepurchase = round(((purchase_list[1]/length_purchase) * 100),2)\n",
    "    perc_removebrowsing = round(((browsing_list[0]/length_browsing) * 100),2)\n",
    "    print(\"Percentage of remove events in abandon: {}, purchase: {}, browsing: {}\".format(perc_removeabandon, perc_removepurchase, perc_removebrowsing))\n",
    "    \n",
    "    perc_detailabandon = round(((abandon_list[2]/length_abandon) * 100),2)\n",
    "    perc_detailpurchase = round(((purchase_list[2]/length_purchase) * 100),2)\n",
    "    perc_detailbrowsing = round(((browsing_list[1]/length_browsing) * 100),2)\n",
    "    print(\"Percentage of detail events in abandon: {}, purchase: {}, browsing: {}\".format(perc_detailabandon, perc_detailpurchase, perc_detailbrowsing))\n",
    "    \n",
    "    perc_viewabandon = round(((abandon_list[3]/length_abandon) * 100),2)\n",
    "    perc_viewpurchase = round(((purchase_list[3]/length_purchase) * 100),2)\n",
    "    perc_viewbrowsing= round(((browsing_list[2]/length_browsing) * 100),2)\n",
    "    print(\"Percentage of view events in abandon: {}, purchase: {}, browsing: {}\".format(perc_viewabandon, perc_viewpurchase, perc_viewbrowsing))\n",
    "    \n",
    "    add_percentages = [perc_addabandon, perc_addpurchase, 0]\n",
    "    remove_percentages = [perc_removeabandon, perc_removepurchase, perc_removebrowsing]\n",
    "    detail_percentages = [perc_detailabandon, perc_detailpurchase, perc_detailbrowsing]\n",
    "    view_percentages = [perc_viewabandon, perc_viewpurchase, perc_viewbrowsing]\n",
    "    %matplotlib inline\n",
    "    df = pd.DataFrame({'Class': [\"Abandon\", \"Purchase\", \"Browsing-Only\"], 'Add': add_percentages, 'Remove': remove_percentages,\n",
    "                      'Detail': detail_percentages, 'View': view_percentages})\n",
    "    print(df)\n",
    "    df.plot(x='Class', kind='bar', stacked=True)\n",
    "    plt.legend(bbox_to_anchor=(1.0, 1.0))\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsperclass(train_abandon_sessions, train_purchase_sessions, train_browsing_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8107a111",
   "metadata": {},
   "source": [
    "Padding & one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_len = 157\n",
    "X_train = pad_sequences(X_train, padding=\"post\",value=6, maxlen=max_len)\n",
    "X_valid = pad_sequences(X_valid, padding=\"post\", value=6, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, padding=\"post\", value=6, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b63b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to one-hot\n",
    "X_train = tf.one_hot(X_train, depth=7)\n",
    "X_valid = tf.one_hot(X_valid, depth=7)\n",
    "X_test = tf.one_hot(X_test, depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to arrays\n",
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "#One-hot encode labels\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_valid = to_categorical(y_valid, 3)\n",
    "\n",
    "# Save for metric calculations\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0716e",
   "metadata": {},
   "source": [
    "Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec8966",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnngridsearch(filter_size, neurons, kernel_size):\n",
    "    batch = 32\n",
    "    epochs = 20\n",
    "    patience = 5\n",
    "    l = 0.001\n",
    "    #Hyperparameters\n",
    "    opt = keras.optimizers.Adam(l)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # Convolutional layer\n",
    "    model.add(Conv1D(filter_size, kernel_size, activation='relu'))\n",
    "    # Pooling layer\n",
    "    model.add(MaxPooling1D(pool_size = 2))    \n",
    "    # Flatten layer\n",
    "    model.add(Flatten())\n",
    "    # Fully connected layer\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4424b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to be tested\n",
    "neurons = [32, 64, 128]\n",
    "kernel_num = [3, 5, 7]\n",
    "filter_num = [32, 64, 128]\n",
    "f1_scores = dict()\n",
    "\n",
    "for n in neurons:\n",
    "    print(\"Testing neurons:\", n)\n",
    "    for k in kernel_num:\n",
    "        print(\"Testing kernel size:\", k)\n",
    "        for f in filter_num:\n",
    "            print(\"Testing filters\", f)\n",
    "            print(\"Fitting model\")\n",
    "            cnn = cnngridsearch(f, n, k)\n",
    "            \n",
    "            #Calculating y_pred\n",
    "            y_pred_validate = cnn.predict(X_valid)\n",
    "            rounded = np.argmax(np.round(y_pred_validate),axis=1)\n",
    "            rounded = list(rounded)\n",
    "        \n",
    "            #Evaluating the model\n",
    "            f1score = f1_score(valid_labels, rounded, average = \"macro\")\n",
    "            print(\"Macro-averaged F1 score:\", f1score)\n",
    "        \n",
    "            #Appending evaluations to dictionaries\n",
    "            f1_scores[f1score] = (f, n, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81218c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort F1 scores from highest to lowest\n",
    "sortscores = {key: val for key, val in sorted(f1_scores.items(), key = lambda ele: ele[0])}\n",
    "print(\"Result dictionary sorted by F1 score : \" + str(sortscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4331b",
   "metadata": {},
   "source": [
    "TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcngridsearch(filter_size, kernel_size, dilations):\n",
    "    epochs=20\n",
    "    patience=5\n",
    "    batch=32\n",
    "    l=0.001\n",
    "    \n",
    "    #Hyperparameters\n",
    "    opt = keras.optimizers.Adam(l)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "\n",
    "    # Define Model\n",
    "    model = keras.Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # TCN layer\n",
    "    model.add(TCN(\n",
    "        nb_filters= filter_size,\n",
    "        kernel_size=kernel_size,\n",
    "        dilations=dilations\n",
    "        ))\n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_num = [3, 5, 7]\n",
    "filter_num = [32, 64, 128]\n",
    "dilations = [[1, 2, 4, 8, 16, 32, 64], [1, 2, 4, 8, 16, 32]]\n",
    "f1_scores = dict()\n",
    "\n",
    "# Calculate receptive field\n",
    "print(\"Receptive fields\")\n",
    "print(\"For kernel size 3: {}\".format(3*1*64))\n",
    "print(\"For kernel size 5: {}\".format(5*1*32))\n",
    "print(\"For kernel size 7: {}\".format(7*1*32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e04ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in filter_num:\n",
    "    print(\"Testing filter size:\", f)\n",
    "    for k in kernel_num:\n",
    "        print(\"Testing kernel size:\", k)\n",
    "        if k == 3:\n",
    "            d = dilations[0]\n",
    "            print(\"Last dilation:\", d[-1])\n",
    "        if k == 5 or k == 7:\n",
    "            d = dilations[1]\n",
    "            print(\"Last dilation:\", d[-1])\n",
    "        \n",
    "        print(\"Fitting model\")\n",
    "        tcn = tcngridsearch(f, k, d)\n",
    "            \n",
    "        #Calculating y_pred\n",
    "        y_pred_validate = tcn.predict(X_valid)\n",
    "        rounded = np.argmax(np.round(y_pred_validate),axis=1)\n",
    "        rounded = list(rounded)\n",
    "        \n",
    "        #Evaluating the model\n",
    "        f1score = f1_score(valid_labels, rounded, average = \"macro\")\n",
    "        print(\"f1 score for this model:\", f1score)\n",
    "        \n",
    "        #Appending evaluations to dictionaries\n",
    "        f1_scores[f1score] = (f, k, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort F1 scores from highest to lowest\n",
    "sortscores = {key: val for key, val in sorted(f1_scores.items(), key = lambda ele: ele[0])}\n",
    "print(\"Result dictionary sorted by F1 score : \" + str(sortscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda22f21",
   "metadata": {},
   "source": [
    "Testing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7396a2f",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores and configure confusion matrices\n",
    "def metric_calculation(predictions):\n",
    "    rounded = np.argmax(np.round(predictions),axis=1)\n",
    "    rounded = list(rounded)\n",
    "    f1 = f1_score(test_labels, rounded, average='macro')\n",
    "    print (\"f1macro: {}\".format(round(f1, 3)))\n",
    "    print(metrics.classification_report(test_labels, rounded, digits=3))\n",
    "    \n",
    "    target_names = [\"Abandon\", \"Purchase\", \"Browsing-only\"]\n",
    "    cm = metrics.confusion_matrix(test_labels, rounded)\n",
    "    cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    heatmap = sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=target_names, yticklabels=target_names, cmap=plt.cm.Blues)\n",
    "    plt.ylabel('Actual class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    figure = heatmap.get_figure()    \n",
    "    figure.savefig('cm.png')\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca5de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "def lstmmodel(X_train, y_train, X_val, y_val):\n",
    "    #Hyperparamaters\n",
    "    lr = 0.001\n",
    "    batch = 32\n",
    "    epochs = 50\n",
    "    patience = 10\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # LSTM layer\n",
    "    model.add(LSTM(64)) \n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6199d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmpredictions(X_test):\n",
    "    lstm = lstmmodel(X_train, y_train, X_valid, y_valid)\n",
    "    preds = lstm.predict(X_test,batch_size=32)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_preds = lstmpredictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_calculation(lstm_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b550c73",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb47eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnmodel(X_train, y_train, X_val, y_val):\n",
    "    lr = 0.001\n",
    "    batch = 32\n",
    "    epochs = 50\n",
    "    patience = 10\n",
    "    opt = keras.optimizers.Adam(learning_rate = lr)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # Convolutional layer\n",
    "    model.add(Conv1D(filters = 32, kernel_size = 7, activation='relu'))\n",
    "    # Pooling layer\n",
    "    model.add(MaxPooling1D(pool_size = 2))    \n",
    "    # Flatten layer\n",
    "    model.add(Flatten())\n",
    "    # Fully connected layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnpredictions(X_test):\n",
    "    cnn = cnnmodel(X_train, y_train, X_valid, y_valid)\n",
    "    preds = cnn.predict(X_test,batch_size=32)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce45cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_preds = cnnpredictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645cc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_calculation(cnn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d07b06",
   "metadata": {},
   "source": [
    "CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstmmodel(X_train, y_train, X_val, y_val):\n",
    "    lr = 0.001\n",
    "    batch = 32\n",
    "    epochs = 50\n",
    "    patience = 10\n",
    "    opt = keras.optimizers.Adam(learning_rate = lr)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # Convolutional layer\n",
    "    model.add(Conv1D(filters = 32, kernel_size = 7, activation='relu'))\n",
    "    # Pooling layer\n",
    "    model.add(MaxPooling1D(pool_size = 2))  \n",
    "    #LSTM layer\n",
    "    model.add(LSTM(64))\n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ebe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstmpredictions(X_test):\n",
    "    cnnlstm = cnnlstmmodel(X_train, y_train, X_valid, y_valid)\n",
    "    preds = cnnlstm.predict(X_test,batch_size=32)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlstm_preds = cnnlstmpredictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63282d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_calculation(cnnlstm_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64967183",
   "metadata": {},
   "source": [
    "TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcnmodel(X_train, y_train, X_val, y_val):\n",
    "    epochs=50\n",
    "    patience=10 \n",
    "    batch=32\n",
    "    lr=0.001 \n",
    "    opt = keras.optimizers.Adam(learning_rate = lr)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                       patience=patience,\n",
    "                                       verbose=1,\n",
    "                                       restore_best_weights=True)\n",
    "\n",
    "    # Define Model\n",
    "    model = keras.Sequential()\n",
    "    model.add(Input(shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(TCN(\n",
    "        nb_filters=64,\n",
    "        kernel_size=7,\n",
    "        dilations=[1, 2, 4, 8, 16, 32]\n",
    "        ))\n",
    "    model.add(keras.layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch,\n",
    "                    callbacks = es)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcnpredictions(X_test):\n",
    "    tcn = tcnmodel(X_train, y_train, X_valid, y_valid)\n",
    "    preds = tcn.predict(X_test,batch_size=32)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_preds = tcnpredictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cefff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_calculation(tcn_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
